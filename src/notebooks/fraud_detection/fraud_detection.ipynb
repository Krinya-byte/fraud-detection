{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q pandas numpy matplotlib seaborn xgboost scikit-learn optuna imblearn umap-learn lightgbm imbalanced-learn boto3 sagemaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "import optuna\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import csv\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from umap import UMAP\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split \n",
    "import optuna\n",
    "from imblearn.over_sampling import SMOTE \n",
    "import numpy as \n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score \n",
    "from imxgboost.imbalance_xgb import imbalance_xgboost as imb_xgb\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAY_ID_ENCR;CUST_LEVEL;GENDER_ENCR;R_AGE_Y;INSTALMENT_IND;INSTAL_CNT;PRODUCT_NAME;MANUFACTURER_NAME_EN;\n",
    "OPERATING_SYSTEM;HANDSET_FEATURE_CAT_DESC;MOVING_AVERAGE_PRICE_AMT_ENCR;SELLING_PRICE_AMT_ENCR;UPFRONT_PYM_AMT_ENCR;monthly_fee_ENCR;\n",
    "TARIFF_LEVEL;CHANNEL_CLASS;channel_group;ADDRESS_COUNTY_ENCR;OUTLET_COUNTY_ENCR;FRAUD_STATUS_6MONTH;NO_PAY_AFTER_12_MONTH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    df_cp = df.copy(deep=True)\n",
    "\n",
    "    #rename cols to lower case\n",
    "    df_cp.columns = df_cp.columns.str.lower()\n",
    "\n",
    "    #drop unnecesarry string cols\n",
    "    #convert to day time\n",
    "    df_cp.day_id_encr = pd.to_datetime(df_cp.day_id_encr, format='%Y. %m. %d.', errors='coerce')\n",
    "    df_cp['year'] = df_cp['day_id_encr'].dt.year\n",
    "    df_cp['month'] = df_cp['day_id_encr'].dt.month\n",
    "    df_cp['day'] = df_cp['day_id_encr'].dt.day\n",
    "    df_cp = df_cp.drop(columns=['day_id_encr'])\n",
    "    # Type Conversion and filling NaNs\n",
    "    numeric_object_cols = df_cp.select_dtypes(include=np.float64)\n",
    "    for col in numeric_object_cols:\n",
    "        df_cp[col] = pd.to_numeric(df_cp[col], errors='coerce')\n",
    "        if df_cp[col].isnull().any():\n",
    "            median_val = df_cp[col].median()\n",
    "            df_cp[col] = df_cp[col].fillna(median_val)\n",
    "\n",
    "    #filling up the age column\n",
    "    df_cp.r_age_y = pd.to_numeric(df_cp.r_age_y, errors='coerce')\n",
    "\n",
    "    # Imputation\n",
    "    categorical_cols = df_cp.select_dtypes(include=['object'])\n",
    "    for col in categorical_cols:\n",
    "        if col in df_cp.columns:\n",
    "            if df_cp[col].isnull().any():\n",
    "                 df_cp[col] = df_cp[col].fillna('Unknown')\n",
    "   \n",
    "    #Boolean convertion\n",
    "    df_cp.fraud_status_6month = df_cp.fraud_status_6month.apply(lambda x: 1 if x == 'Y' else 0) \n",
    "    df_cp.instalment_ind = df_cp.instalment_ind.apply(lambda x: 1 if x =='Y' else 0)\n",
    "\n",
    "    return df_cp\n",
    "\n",
    "def apply_log_transform(df, cols_to_transform):\n",
    "    df_transformed = df.copy()\n",
    "\n",
    "    for col in cols_to_transform:\n",
    "        df_transformed[col] = np.log1p(df_transformed[col])\n",
    "\n",
    "    return df_transformed\n",
    "\n",
    "# def fit_umap_with_base_params(X_train, n_components=2, n_neighbors=15, min_dist=0.1, metric='euclidean'):\n",
    "    \n",
    "#     umap_model = UMAP(\n",
    "#         n_components=n_components,\n",
    "#         n_neighbors=n_neighbors,\n",
    "#         min_dist=min_dist,\n",
    "#         metric=metric,\n",
    "#         random_state=DEFAULT_RANDOM_STATE,\n",
    "#         transform_seed=DEFAULT_RANDOM_STATE\n",
    "#     )\n",
    "    \n",
    "#     X_train_embeddings = umap_model.fit_transform(X_train)\n",
    "    \n",
    "#     return X_train_embeddings\n",
    "\n",
    "def save_model_to_s3(model,s3_bucket,s3_key):\n",
    "\n",
    "    local_model_filename = \"model.xgb\"  \n",
    "    local_tarball_name = \"model.tar.gz\"   \n",
    "\n",
    "    try:\n",
    "       \n",
    "        model.save_model(local_model_filename)\n",
    "\n",
    "        \n",
    "        with tarfile.open(local_tarball_name, \"w:gz\") as tar:\n",
    "    \n",
    "            tar.add(local_model_filename)\n",
    "\n",
    "        s3_client = boto3.client(\"s3\")\n",
    "        s3_client.upload_file(local_tarball_name, s3_bucket, s3_key)\n",
    "        \n",
    "        model_s3_uri = f\"s3://{s3_bucket}/{s3_key}\"\n",
    "        print(f\"Model successfully packaged and uploaded to: {model_s3_uri}\")\n",
    "        return model_s3_uri\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model packaging or S3 upload: {e}\")\n",
    "        return None\n",
    "\n",
    "    finally:\n",
    "        if os.path.exists(local_model_filename):\n",
    "            os.remove(local_model_filename)\n",
    "        if os.path.exists(local_tarball_name):\n",
    "            os.remove(local_tarball_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_optuna_study(study, filename, save_dir='optuna_studies'):\n",
    "    \"\"\"Saves an Optuna study object to a file using joblib.\"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        print(f\"Created directory: {save_dir}\")\n",
    "        \n",
    "    filepath = os.path.join(save_dir, filename)\n",
    "    try:\n",
    "        joblib.dump(study, filepath)\n",
    "        print(f\"Successfully saved Optuna study to: {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving Optuna study to {filepath}: {e}\")\n",
    "\n",
    "def load_optuna_study(filename):\n",
    "    return joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = '../../../dataset.csv' \n",
    "df = pd.read_csv(data_path,decimal=',', delimiter=';', quoting=csv.QUOTE_NONE, encoding='utf-8-sig',encoding_errors='ignore', date_parser=lambda x: pd.to_datetime(x, format='%Y.%m.%d.'), na_values='?')\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing data columns in case of local run\n",
    "df_processed = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_processed.fillna(0,inplace=True)\n",
    "numeric_cols_for_eda = df_processed.select_dtypes(include=np.float64).columns \n",
    "cols_to_remove_from_numeric = ['r_age_y', 'instal_cnt','gender_encr']\n",
    "numeric_cols_for_eda = [col for col in numeric_cols_for_eda if col not in cols_to_remove_from_numeric]\n",
    "numeric_cols_for_eda_df = df_processed[numeric_cols_for_eda]\n",
    "correlation_matrix = numeric_cols_for_eda_df.corr()\n",
    "\n",
    "plt.figure(figsize=(14, 14))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Matrix of Numeric Features')\n",
    "plt.show()\n",
    "skewness = numeric_cols_for_eda_df.skew()\n",
    "\n",
    "print(\"Skewness of numeric features:\")\n",
    "print(skewness)\n",
    "\n",
    "for col in numeric_cols_for_eda_df.columns:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.histplot(df_processed[col], kde=True)\n",
    "        skew_val = skewness.get(col, float('nan'))\n",
    "        plt.title(f'Distribution of {col} (Skewness: {skew_val:.2f})')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "\n",
    "skew_threshold = 1\n",
    "\n",
    "skewed_features_identified_from_eda = skewness[skewness > skew_threshold].index.tolist()\n",
    "\n",
    "\n",
    "target_cols = ['fraud_status_6month', 'no_pay_after_12_month']\n",
    "\n",
    "all_numeric_column_names = df_processed.drop(columns=target_cols, errors='ignore') \\\n",
    "                                   .select_dtypes(include=np.float64).columns.tolist()\n",
    "\n",
    "cols_to_remove_from_numeric = ['r_age_y', 'instal_cnt','gender_encr']\n",
    "all_numeric_column_names = [col for col in all_numeric_column_names if col not in cols_to_remove_from_numeric]\n",
    "\n",
    "pipeline_skewed_numeric_features = [\n",
    "    col for col in skewed_features_identified_from_eda if col in all_numeric_column_names\n",
    "]\n",
    "\n",
    "pipeline_non_skewed_numeric_features = [\n",
    "    col for col in all_numeric_column_names if col not in pipeline_skewed_numeric_features\n",
    "]\n",
    "\n",
    "text_feature_colname = 'product_name'\n",
    "ordinal_feature_colnames = ['cust_level', 'handset_feature_cat_desc', 'tariff_level']\n",
    "\n",
    "all_object_cols = df_processed.select_dtypes(include='object').columns.tolist()\n",
    "ohe_feature_colnames = [\n",
    "    col for col in all_object_cols\n",
    "    if col != text_feature_colname and col not in ordinal_feature_colnames\n",
    "]\n",
    "\n",
    "def log1p_safe(x_series):\n",
    "    x_clipped = np.maximum(x_series, -1.0 + 1e-9)\n",
    "    \n",
    "    return np.log1p(x_clipped)\n",
    "\n",
    "skewed_numeric_transformer_with_scale = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('logtransform', FunctionTransformer(log1p_safe, validate=False)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "non_skewed_numeric_transformer_with_scale = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "skewed_numeric_transformer_no_scale = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('logtransform', FunctionTransformer(np.log1p, validate=False))\n",
    "])\n",
    "\n",
    "non_skewed_numeric_transformer_no_scale = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "text_transformer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english', max_features=100)\n",
    "\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('ordinalencoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "nominal_ohe_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='error', sparse_output=False))\n",
    "])\n",
    "\n",
    "actual_skewed_numeric_features = [col for col in pipeline_skewed_numeric_features if col in df_processed.columns]\n",
    "actual_non_skewed_numeric_features = [col for col in pipeline_non_skewed_numeric_features if col in df_processed.columns]\n",
    "actual_ordinal_cols = [col for col in ordinal_feature_colnames if col in df_processed.columns]\n",
    "actual_ohe_cols = [col for col in ohe_feature_colnames if col in df_processed.columns]\n",
    "\n",
    "transformers_config_with_scaling = [\n",
    "    ('skewed_num', skewed_numeric_transformer_with_scale, actual_skewed_numeric_features),\n",
    "    ('non_skewed_num', non_skewed_numeric_transformer_with_scale, actual_non_skewed_numeric_features),\n",
    "    ('text', text_transformer, text_feature_colname),\n",
    "    ('ordinal', ordinal_transformer, actual_ordinal_cols),\n",
    "    ('onehot', nominal_ohe_transformer, actual_ohe_cols)  \n",
    "]\n",
    "\n",
    "preprocessor_with_scaling = ColumnTransformer(\n",
    "    transformers=transformers_config_with_scaling,\n",
    "    remainder='drop',\n",
    "    verbose_feature_names_out=False \n",
    ")\n",
    "\n",
    "\n",
    "transformers_config_no_scaling = [\n",
    "    ('skewed_num', skewed_numeric_transformer_no_scale, actual_skewed_numeric_features),\n",
    "    ('non_skewed_num', non_skewed_numeric_transformer_no_scale, actual_non_skewed_numeric_features),\n",
    "    ('text', text_transformer, text_feature_colname),\n",
    "    ('ordinal', ordinal_transformer, actual_ordinal_cols), \n",
    "    ('onehot', nominal_ohe_transformer, actual_ohe_cols)  \n",
    "]\n",
    "\n",
    "preprocessor_no_scaling = ColumnTransformer(\n",
    "    transformers=transformers_config_no_scaling,\n",
    "    remainder='drop',\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "df_features_only = df_processed.drop(columns=target_cols, errors='ignore')\n",
    "\n",
    "\n",
    "X_processed_for_logreg = preprocessor_with_scaling.fit_transform(df_features_only)\n",
    "\n",
    "X_processed_for_others = preprocessor_no_scaling.fit_transform(df_features_only)\n",
    "\n",
    "\n",
    "y_fraud_6month = df_processed['fraud_status_6month'].astype(int)\n",
    "y_nopay = df_processed['no_pay_after_12_month'].astype(int)\n",
    "\n",
    "print(f\"Shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_processed_for_others, df_processed.fraud_status_6month, test_size=0.2, random_state=42, stratify= df_processed.fraud_status_6month\n",
    ")\n",
    "\n",
    "X_train_no_pay, X_val_no_pay, y_train_no_pay, y_val_no_pay = train_test_split(\n",
    "    X_processed_for_others, df_processed.no_pay_after_12_month, test_size=0.2, random_state=42, stratify=df_processed.no_pay_after_12_month\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.to_numpy()\n",
    "y_train_no_pay = y_train_no_pay.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train_lin_Reg_fraud, X_val_lin_Reg_fraud, y_train_lin_Reg_fraud, y_val_lin_Reg_fraud = train_test_split(\n",
    "    X_processed_for_logreg,\n",
    "    df_processed.fraud_status_6month, \n",
    "    test_size=0.2,\n",
    "    random_state=42, \n",
    "    stratify=df_processed.fraud_status_6month\n",
    ")\n",
    "\n",
    "X_train_lin_Reg_no_pay, X_val_lin_Reg_no_pay, y_train_lin_Reg_no_pay, y_val_lin_Reg_no_pay = train_test_split(\n",
    "    X_processed_for_logreg, \n",
    "    df_processed.no_pay_after_12_month, \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_processed.no_pay_after_12_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "X_train_no_pay_smote, y_train_no_pay_smote = smote.fit_resample(X_train_no_pay, y_train_no_pay)\n",
    "\n",
    "\n",
    "X_train_lin_Reg_smote, y_train_lin_Reg_smote = smote.fit_resample(X_train_lin_Reg_fraud, y_train_lin_Reg_fraud)\n",
    "\n",
    "\n",
    "X_train_lin_Reg_smote_no_pay, y_train_lin_Reg_smote_no_pay = smote.fit_resample(X_train_lin_Reg_no_pay, y_train_lin_Reg_no_pay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=y_fraud_6month)\n",
    "plt.title('Class Distribution for fraud_status_6month')\n",
    "plt.xlabel('Fraud Status (6 months)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=y_nopay)\n",
    "plt.title('Class Distribution for no_pay_after_12_month')\n",
    "plt.xlabel('No Pay After 12 Months')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "print(\"Value counts for y_fraud_6month:\")\n",
    "print(y_fraud_6month.value_counts(normalize=True))\n",
    "print(\"\\nValue counts for y_no_pay_after_12_month:\")\n",
    "print(y_nopay.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cv_stratified = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "def objective_xgb_optuna(trial, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Optuna objective function for XGBClassifier.\n",
    "    Optimizes F1-score using cross-validation.\n",
    "    Handles class imbalance using scale_pos_weight.\n",
    "    \"\"\"\n",
    "\n",
    "   \n",
    "    xgb_params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True), \n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1.0, 100.0, log=True),\n",
    "        'random_state': 42,\n",
    "        'objective': 'binary:logistic', \n",
    "        'use_label_encoder': False, \n",
    "        'eval_metric': 'logloss' \n",
    "        }\n",
    "\n",
    "    \n",
    "    model = xgb.XGBClassifier(**xgb_params)\n",
    "\n",
    "    \n",
    "    score = cross_val_score(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=cv_stratified,\n",
    "        scoring='f1', \n",
    "        n_jobs=-1,\n",
    "        error_score='raise'\n",
    "    )\n",
    "\n",
    "    return score.mean()\n",
    "\n",
    "\n",
    "def objective_logreg_smote_configurable(trial,X_train_val,y_train_val):\n",
    "    logreg_c = trial.suggest_float('C', 1e-4, 1e4, log=True)\n",
    "    logreg_penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])\n",
    "    \n",
    "    if logreg_penalty == 'l1':\n",
    "        logreg_solver = trial.suggest_categorical('solver', ['liblinear', 'saga'])\n",
    "    else:\n",
    "        logreg_solver = trial.suggest_categorical('solver', ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'])\n",
    "\n",
    "    logreg_model = LogisticRegression(\n",
    "        C=logreg_c,\n",
    "        penalty=logreg_penalty,\n",
    "        solver=logreg_solver,\n",
    "        random_state=42,\n",
    "        max_iter=1000 \n",
    "    )\n",
    "    \n",
    "    score = cross_val_score(\n",
    "        logreg_model, \n",
    "        X_train_val,\n",
    "        y_train_val, \n",
    "        cv=cv_stratified, \n",
    "        scoring='f1', \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    return score.mean()\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def objective_sgd(trial,x_train,y_train):\n",
    "    sgd_loss = trial.suggest_categorical('loss', ['hinge', 'log_loss', 'modified_huber'])\n",
    "    sgd_penalty = trial.suggest_categorical('penalty', ['l2', 'l1', 'elasticnet'])\n",
    "    sgd_alpha = trial.suggest_float('alpha', 1e-6, 1e-1, log=True) \n",
    "    sgd_l1_ratio = 0.15 \n",
    "    if sgd_penalty == 'elasticnet':\n",
    "        sgd_l1_ratio = trial.suggest_float('l1_ratio', 0.0, 1.0)\n",
    "        \n",
    "    sgd_learning_rate = 'optimal' \n",
    "\n",
    "    model = SGDClassifier(\n",
    "        loss=sgd_loss,\n",
    "        penalty=sgd_penalty,\n",
    "        alpha=sgd_alpha,\n",
    "        l1_ratio=sgd_l1_ratio,\n",
    "        learning_rate=sgd_learning_rate,\n",
    "        max_iter=trial.suggest_int('max_iter', 500, 3000, step=500), \n",
    "        tol=1e-3, \n",
    "        shuffle=True, \n",
    "        random_state=42,\n",
    "        class_weight='balanced', \n",
    "        n_jobs=-1 \n",
    "    )\n",
    "    \n",
    "    score = cross_val_score(model, x_train, y_train, cv=cv_stratified, scoring='f1', n_jobs=-1)    \n",
    "    return score.mean()\n",
    "\n",
    "\n",
    "def objective_knn_configurable(trial, x_train,y_train):\n",
    "    knn_n_neighbors = trial.suggest_int('n_neighbors', 1, 50)\n",
    "    knn_weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "    knn_metric = trial.suggest_categorical('metric', ['euclidean', 'manhattan'])\n",
    "    \n",
    "\n",
    "    knn_model = KNeighborsClassifier(\n",
    "        n_neighbors=knn_n_neighbors,\n",
    "        weights=knn_weights,\n",
    "        metric=knn_metric,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "        \n",
    "    score = cross_val_score(knn_model, X_train, y_train, cv=cv_stratified, scoring='f1', n_jobs=-1)\n",
    "    return score.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_N_OPTUNA_TRIALS=50\n",
    "\n",
    "print(\"\\nStarting Optuna for Logistic Regression (No SMOTE)...\")\n",
    "study_logreg_no_smote_fraud = optuna.create_study(\n",
    "    direction='maximize', \n",
    "    study_name='LogisticRegression_NoSMOTE_F1_fraud'\n",
    ")\n",
    "study_logreg_no_smote_no_pay = optuna.create_study(\n",
    "    direction='maximize', \n",
    "    study_name='LogisticRegression_NoSMOTE_F1_no_pay'\n",
    ")\n",
    "\n",
    "study_logreg_no_smote_fraud.optimize(\n",
    "    lambda trial: objective_logreg_smote_configurable(trial, X_train_lin_Reg_fraud,y_train_lin_Reg_fraud), \n",
    "    n_trials=50, \n",
    "    timeout=600\n",
    ")\n",
    "study_logreg_no_smote_no_pay.optimize(\n",
    "    lambda trial: objective_logreg_smote_configurable(trial, X_train_lin_Reg_no_pay,y_train_lin_Reg_no_pay), \n",
    "    n_trials=50, \n",
    "    timeout=600\n",
    ")\n",
    "print(\"\\nBest F1 for Logistic Regression (No SMOTE) fraud:\", study_logreg_no_smote_fraud.best_value)\n",
    "print(\"\\nBest F1 for Logistic Regression (No SMOTE) no pay:\", study_logreg_no_smote_no_pay.best_value)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nStarting Optuna for Logistic Regression (WITH SMOTE)...\")\n",
    "study_logreg_smote_fraud = optuna.create_study(\n",
    "    direction='maximize', \n",
    "    study_name='LogisticRegression_SMOTE_F1_fraud'\n",
    ")\n",
    "study_logreg_smote_no_pay = optuna.create_study(\n",
    "    direction='maximize', \n",
    "    study_name='LogisticRegression_SMOTE_F1_no_pay'\n",
    ")\n",
    "study_logreg_smote_fraud.optimize(\n",
    "    lambda trial: objective_logreg_smote_configurable(trial, X_train_lin_Reg_smote,y_train_lin_Reg_smote), \n",
    "    n_trials=DEFAULT_N_OPTUNA_TRIALS, \n",
    "    timeout=700 \n",
    ")\n",
    "study_logreg_smote_no_pay.optimize(\n",
    "    lambda trial: objective_logreg_smote_configurable(trial, X_train_lin_Reg_smote_no_pay,y_train_lin_Reg_smote_no_pay), \n",
    "    n_trials=DEFAULT_N_OPTUNA_TRIALS, \n",
    "    timeout=700 \n",
    ")\n",
    "\n",
    "print(\"\\nBest F1 for Logistic Regression (WITH SMOTE):\", study_logreg_smote_fraud.best_value)\n",
    "print(\"\\nBest F1 for Logistic Regression (WITH SMOTE):\", study_logreg_smote_no_pay.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_optuna_study(study_logreg_no_smote_fraud,'study_logreg_no_smote_fraud.pkl')\n",
    "save_optuna_study(study_logreg_no_smote_no_pay,'study_logreg_no_smote_no_pay.pkl')\n",
    "save_optuna_study(study_logreg_smote_fraud,'study_logreg_smote_fraud.pkl')\n",
    "save_optuna_study(study_logreg_smote_no_pay,'study_logreg_smote_no_pay.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\nStarting Optuna for XGBoost with Focal Loss SMOTE\")\n",
    "study_xgb_fraud_smote = optuna.create_study(direction='maximize', study_name='XGBoost_fraud_SMOTE')\n",
    "study_xgb_fraud_smote.optimize(lambda trial: objective_xgb_optuna(trial,X_train_smote,y_train_smote), n_trials=DEFAULT_N_OPTUNA_TRIALS, timeout=700) \n",
    "study_xgb_no_pay_smote = optuna.create_study(direction='maximize', study_name='XGBoost_no_pay_SMOTE')\n",
    "study_xgb_no_pay_smote.optimize(lambda trial: objective_xgb_optuna(trial,X_train_no_pay_smote,y_train_no_pay_smote), n_trials=DEFAULT_N_OPTUNA_TRIALS, timeout=700) \n",
    "print(\"\\nBest F1 for XGBoost (Actual Focal Loss) fraud SMOTE:\", study_xgb_fraud_smote.best_value)\n",
    "print(\"\\nBest F1 for XGBoost (Actual Focal Loss) no_pay SMOTE:\", study_xgb_no_pay_smote.best_value)\n",
    "\n",
    "print(\"\\nStarting Optuna for XGBoost with Focal Loss (No SMOTE)\")\n",
    "study_xgb_fraud_no_smote = optuna.create_study(direction='maximize', study_name='XGBoost_fraud_NoSMOTE')\n",
    "study_xgb_fraud_no_smote.optimize(lambda trial: objective_xgb_optuna(trial,X_train,y_train), n_trials=DEFAULT_N_OPTUNA_TRIALS, timeout=700) \n",
    "study_xgb_no_pay_no_smote = optuna.create_study(direction='maximize', study_name='XGBoost_no_pay_NoSMOTE')\n",
    "study_xgb_no_pay_no_smote.optimize(lambda trial: objective_xgb_optuna(trial,X_train_no_pay,y_train_no_pay), n_trials=DEFAULT_N_OPTUNA_TRIALS, timeout=700) \n",
    "print(\"\\nBest F1 for XGBoost (Actual Focal Loss) fraud (No SMOTE):\", study_xgb_fraud_no_smote.best_value)\n",
    "print(\"\\nBest F1 for XGBoost (Actual Focal Loss) no_pay (No SMOTE):\", study_xgb_no_pay_no_smote.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_optuna_study(study_xgb_fraud_smote,'study_xgb_fraud_smote.pkl')\n",
    "save_optuna_study(study_xgb_no_pay_smote,'study_xgb_no_pay_smote.pkl')\n",
    "save_optuna_study(study_xgb_fraud_no_smote,'study_xgb_fraud_no_smote.pkl')\n",
    "save_optuna_study(study_xgb_no_pay_no_smote,'study_xgb_no_pay_no_smote.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting Optuna for SGDClassifier (No SMOTE)...\")\n",
    "study_sgd_fraud_no_smote = optuna.create_study(direction='maximize', study_name='SGDClassifier_NoSMOTE_F1_fraud')\n",
    "study_sgd_fraud_no_smote.optimize(\n",
    "    lambda trial: objective_sgd(trial, X_train, y_train),\n",
    "    n_trials=DEFAULT_N_OPTUNA_TRIALS,\n",
    "    timeout=600\n",
    ")\n",
    "study_sgd_no_pay_no_smote = optuna.create_study(direction='maximize', study_name='SGDClassifier_NoSMOTE_F1_no_pay')\n",
    "study_sgd_no_pay_no_smote.optimize(\n",
    "    lambda trial: objective_sgd(trial, X_train_no_pay, y_train_no_pay), \n",
    "    n_trials=DEFAULT_N_OPTUNA_TRIALS,\n",
    "    timeout=600\n",
    ")\n",
    "print(\"\\nBest F1 for SGDClassifier (No SMOTE) fraud:\", study_sgd_fraud_no_smote.best_value)\n",
    "print(\"Best params for SGDClassifier (No SMOTE) no_pay:\", study_sgd_no_pay_no_smote.best_value) \n",
    "\n",
    "# Optuna Study for SGDClassifier WITH SMOTE\n",
    "print(\"\\nStarting Optuna for SGDClassifier (WITH SMOTE)...\")\n",
    "study_sgd_fraud_smote = optuna.create_study(direction='maximize', study_name='SGDClassifier_SMOTE_F1_fraud')\n",
    "study_sgd_fraud_smote.optimize(\n",
    "    lambda trial: objective_sgd(trial,X_train_smote,y_train_smote),\n",
    "    n_trials=DEFAULT_N_OPTUNA_TRIALS,\n",
    "    timeout=600\n",
    ")\n",
    "study_sgd_no_pay_smote = optuna.create_study(direction='maximize', study_name='SGDClassifier_SMOTE_F1_no_pay')\n",
    "study_sgd_no_pay_smote.optimize(\n",
    "    lambda trial: objective_sgd(trial, X_train_no_pay_smote, y_train_no_pay_smote),\n",
    "    n_trials=DEFAULT_N_OPTUNA_TRIALS,\n",
    "    timeout=600\n",
    ")\n",
    "print(\"\\nBest F1 for SGDClassifier (WITH SMOTE) fraud:\", study_sgd_fraud_smote.best_value)\n",
    "print(\"Best params for SGDClassifier (WITH SMOTE) no_pay:\", study_sgd_no_pay_smote.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_optuna_study(study_sgd_fraud_no_smote,'study_sgd_fraud_no_smote.pkl')\n",
    "save_optuna_study(study_sgd_no_pay_no_smote,'study_sgd_fraud_no_pay_no_smote.pkl')\n",
    "save_optuna_study(study_sgd_fraud_smote,'study_sgd_fraud_smote.pkl')\n",
    "save_optuna_study(study_sgd_no_pay_smote,'study_sgd_fraud_no_pay_smote.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna Study for KNN WITHOUT SMOTE\n",
    "print(\"\\nStarting Optuna for KNN (No SMOTE)...\")\n",
    "study_knn_fraud_no_smote = optuna.create_study(direction='maximize', study_name='KNN_NoSMOTE_F1_fraud')\n",
    "study_knn_fraud_no_smote.optimize(\n",
    "    lambda trial: objective_knn_configurable(trial, X_train,y_train),\n",
    "    n_trials=DEFAULT_N_OPTUNA_TRIALS, \n",
    "    timeout=600\n",
    ")\n",
    "study_knn_no_pay_no_smote = optuna.create_study(direction='maximize', study_name='KNN_NoSMOTE_F1_no_pay')\n",
    "study_knn_no_pay_no_smote.optimize(\n",
    "    lambda trial: objective_knn_configurable(trial,  X_train,y_train),\n",
    "    n_trials=DEFAULT_N_OPTUNA_TRIALS, \n",
    "    timeout=600\n",
    ")\n",
    "print(\"\\nBest F1 for KNN (No SMOTE) fraud:\", study_knn_fraud_no_smote.best_value)\n",
    "print(\"Best params for KNN (No SMOTE) no_pay:\", study_knn_no_pay_no_smote.best_value)\n",
    "\n",
    "save_optuna_study(study_knn_fraud_no_smote,'knn_fraud_no_smote.pkl')\n",
    "# Optuna Study for KNN WITH SMOTE\n",
    "print(\"\\nStarting Optuna for KNN (WITH SMOTE)...\")\n",
    "study_knn_smote_fraud = optuna.create_study(direction='maximize', study_name='KNN_SMOTE_F1_fraud')\n",
    "study_knn_smote_fraud.optimize(\n",
    "    lambda trial: objective_knn_configurable(trial, X_train_smote,y_train_smote),\n",
    "    n_trials=DEFAULT_N_OPTUNA_TRIALS, \n",
    "    timeout=600 \n",
    ")\n",
    "\n",
    "study_knn_smote_no_pay = optuna.create_study(direction='maximize', study_name='KNN_SMOTE_F1_no_pay')\n",
    "study_knn_smote_no_pay.optimize(\n",
    "    lambda trial: objective_knn_configurable(trial, X_train_smote,y_train_smote),\n",
    "    n_trials=DEFAULT_N_OPTUNA_TRIALS, \n",
    "    timeout=600 \n",
    ")\n",
    "print(\"\\nBest F1 for KNN (WITH SMOTE):\", study_knn_smote_fraud.best_value)\n",
    "print(\"Best params for KNN (WITH SMOTE):\", study_knn_smote_no_pay.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Logistic Regression studies\n",
    "study_logreg_fraud_no_smote = load_optuna_study('./optuna_studies/study_logreg_no_smote_fraud.pkl')\n",
    "study_logreg_no_pay_no_smote = load_optuna_study('./optuna_studies/study_logreg_no_smote_no_pay.pkl')\n",
    "study_logreg_fraud_smote = load_optuna_study('./optuna_studies/study_logreg_smote_fraud.pkl')\n",
    "study_logreg_no_pay_smote = load_optuna_study('./optuna_studies/study_logreg_smote_no_pay.pkl')\n",
    "print(\"Logistic Regression Optuna studies reloaded from .pkl files.\")\n",
    "\n",
    "# Load XGBoost studies\n",
    "study_xgb_fraud_no_smote = load_optuna_study('./optuna_studies/study_xgb_fraud_no_smote.pkl')\n",
    "study_xgb_no_pay_no_smote = load_optuna_study('./optuna_studies/study_xgb_no_pay_no_smote.pkl')\n",
    "study_xgb_fraud_smote = load_optuna_study('./optuna_studies/study_xgb_fraud_smote.pkl')\n",
    "study_xgb_no_pay_smote = load_optuna_study('./optuna_studies/study_xgb_no_pay_smote.pkl')\n",
    "print(\"XGBoost Optuna studies reloaded from .pkl files.\")\n",
    "\n",
    "# Load SGD Classifier studies\n",
    "study_sgd_fraud_no_smote = load_optuna_study('./optuna_studies/study_sgd_fraud_no_smote.pkl')\n",
    "study_sgd_no_pay_no_smote = load_optuna_study('./optuna_studies/study_sgd_fraud_no_pay_no_smote.pkl') \n",
    "study_sgd_no_pay_smote = load_optuna_study('./optuna_studies/study_sgd_fraud_no_pay_smote.pkl')\n",
    "print(\"SGD Classifier Optuna studies reloaded from .pkl files.\")\n",
    "\n",
    "study_knn_fraud_no_smote = load_optuna_study('./optuna_studies/knn_fraud_no_smote.pkl')\n",
    "# study_knn_no_pay_no_smote = load_optuna_study('knn_no_pay_no_smote.pkl')\n",
    "# study_knn_smote_fraud = load_optuna_study('knn_fraud_smote.pkl') \n",
    "# study_knn_smote_no_pay = load_optuna_study('knn_no_pay_smote.pkl') \n",
    "print(\"KNN Optuna studies reloaded from .pkl files.\")\n",
    "\n",
    "print(\"\\nAll specified Optuna studies have been attempted to be reloaded from .pkl files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fraud_models = [\n",
    "    study_logreg_fraud_no_smote,\n",
    "    study_logreg_fraud_smote,\n",
    "    study_xgb_fraud_no_smote,\n",
    "    study_xgb_fraud_smote,\n",
    "    study_sgd_fraud_no_smote,\n",
    "    study_knn_fraud_no_smote\n",
    "]\n",
    "\n",
    "no_pay_models = [\n",
    "    study_logreg_no_pay_no_smote,\n",
    "    study_logreg_no_pay_smote,\n",
    "    study_xgb_no_pay_no_smote,\n",
    "    study_xgb_no_pay_smote,\n",
    "    study_sgd_no_pay_no_smote,\n",
    "    study_sgd_no_pay_smote \n",
    "]\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'LogisticRegression': LogisticRegression,\n",
    "    'XGBoost': XGBClassifier,\n",
    "    'SGDClassifier': SGDClassifier,\n",
    "    'KNN': KNeighborsClassifier\n",
    "}\n",
    "\n",
    "def train_and_evaluate(model_cls, best_params, X_train, y_train, X_test, y_test):\n",
    "    model = model_cls(**best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    score = f1_score(y_test, preds, average='weighted')     \n",
    "    return model, score\n",
    "\n",
    "def validate_and_save_best(X_train, y_train, X_test, y_test, target_list,target_name):\n",
    "    best_score = -1\n",
    "    best_model = None\n",
    "    best_model_name = ''\n",
    "    for study in target_list:\n",
    "        print(f\"Evaluating {study.study_name} for {target_name}\")\n",
    "        try:\n",
    "            best_params = study.best_trial.params\n",
    "            model_name = study.study_name.split(\n",
    "                '_'\n",
    "            )[0]\n",
    "            model, score = train_and_evaluate(MODEL_CLASSES[model_name], best_params, X_train, y_train, X_test, y_test)\n",
    "\n",
    "            print(f\"{model_name} accuracy: {score:.4f}\")\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to evaluate {model_name} for {target_name}: {e}\")\n",
    "\n",
    "    if best_model:\n",
    "        save_model_to_s3(best_model,'yettel-fraud-detection-inference',f'best_model_{target_name}.tar.gz')\n",
    "        print(f\"Saved best model ({best_model_name}) for {target_name} with score {best_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting evaluation for fraud models...\")\n",
    "validate_and_save_best(\n",
    "    X_train, y_train, \n",
    "    X_val, y_val, \n",
    "    fraud_models, \n",
    "    \"fraud\" \n",
    ")\n",
    "print(\"\\nStarting evaluation for no-pay models...\")\n",
    "validate_and_save_best(\n",
    "    X_train_no_pay, y_train_no_pay, \n",
    "    X_val_no_pay, y_val_no_pay, \n",
    "    no_pay_models, \n",
    "    \"no_pay\"  \n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
