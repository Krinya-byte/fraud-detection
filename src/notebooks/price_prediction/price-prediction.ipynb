{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import joblib\n",
    "import math\n",
    "import optuna\n",
    "\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "current_credentials = credentials.get_frozen_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(\n",
    "    aws_access_key_id=current_credentials.access_key,\n",
    "    aws_secret_access_key=current_credentials.secret_key,\n",
    "    aws_session_token=current_credentials.token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_bucket = 'curated-bucket-car-price-tk'\n",
    "file_key = 'ml_sample_data_snapsoft.csv'\n",
    "\n",
    "s3_client = session.client('s3')\n",
    "response = s3_client.get_object(Bucket=curated_bucket, Key=file_key)\n",
    "car_data = pd.read_csv(response['Body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputing float data values with median values\n",
    "numerical_cols = car_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "car_data[numerical_cols] = imputer.fit_transform(car_data[numerical_cols])\n",
    "car_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "#create timestamp column from salesdate\n",
    "car_data['saledate'] = pd.to_datetime(car_data['saledate'])\n",
    "\n",
    "car_data['saledate_timestamp'] = pd.to_datetime(car_data['saledate']).values.astype(np.int64) // 10 ** 9\n",
    "car_data.saledate_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding categorical columns using one hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "categorical_columns = car_data.select_dtypes(include=['object']).columns\n",
    "encoded_data = encoder.fit_transform(car_data[categorical_columns])\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded_data.toarray(), columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "df = car_data.drop(categorical_columns, axis=1)\n",
    "df = pd.concat([df, encoded_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price underestimation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#before training\n",
    "#cutoff the 95 percentile of the training dataset\n",
    "def upper_percentile_cutoff(percentile, source_dataset,destination_dataset):\n",
    "    percentile = np.percentile(source_dataset, percentile)\n",
    "    destination_dataset = np.where(destination_dataset > percentile, percentile, destination_dataset)\n",
    "    return destination_dataset\n",
    "\n",
    "#after training\n",
    "#subtract the 5 percentile of the predictions\n",
    "def lower_percentile_subtract(percentile, source_dataset, destination_dataset):\n",
    "    percentile = np.percentile(source_dataset, percentile)\n",
    "    destination_dataset = destination_dataset - percentile\n",
    "    return destination_dataset\n",
    "\n",
    "#before training\n",
    "#scale the training dataset to a max range\n",
    "def min_max_scale(data, feature_range=(0, 1)):\n",
    "    min_val = feature_range[0]\n",
    "    max_val = feature_range[1]\n",
    "    \n",
    "    data_min = np.min(data)\n",
    "    data_max = np.max(data)\n",
    "    \n",
    "    scaled_data = (data - data_min) / (data_max - data_min) * (max_val - min_val) + min_val\n",
    "    return scaled_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tune_and_train_model(X_train, y_train_scaled, X_test, y_test_scaled):\n",
    "    def objective(trial):\n",
    "        param = {\n",
    "            'objective': 'root_mean_squared_error',\n",
    "            'metric': 'root_mean_squared_error',\n",
    "            'verbosity': -1,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100)\n",
    "        }\n",
    "\n",
    "        train_data = lgb.Dataset(X_train, label=y_train_scaled)\n",
    "        valid_data = lgb.Dataset(X_test, label=y_test_scaled, reference=train_data)\n",
    "\n",
    "        model = lgb.train(param, train_data, valid_sets=[valid_data])\n",
    "        preds = model.predict(X_test)\n",
    "        rmse = mean_squared_error(y_test_scaled, preds)\n",
    "        return rmse\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train_scaled)\n",
    "    valid_data = lgb.Dataset(X_test, label=y_test_scaled, reference=train_data)\n",
    "\n",
    "    model = lgb.train(best_params, train_data, valid_sets=[valid_data])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_and_plot(model,y, X_test, y_test, saledate_y,title,models,after_training_cutoff=None):\n",
    "    predictions = model.predict(X_test)\n",
    "    if after_training_cutoff is not None:\n",
    "        predictions = after_training_cutoff(5,y,predictions)\n",
    "    rmse = math.sqrt(mean_squared_error(y_test, predictions))\n",
    "    print(f'Mean Squared Error: {math.sqrt(rmse)}')\n",
    "\n",
    "    plot_df = pd.DataFrame({\n",
    "        'saledate': saledate_y,\n",
    "        'actual_price': y_test,\n",
    "        'predictions': predictions\n",
    "    })\n",
    "\n",
    "    #save model to a dictionary along with the rmse\n",
    "    models[model] = rmse\n",
    "\n",
    "    plot_df = plot_df.sort_values(by='saledate')\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(plot_df['saledate'], plot_df['actual_price'], label='Actual Prices', marker='o')\n",
    "    plt.plot(plot_df['saledate'], plot_df['predictions'], label='Predictions', marker='x')\n",
    "    plt.xlabel('Sale Date')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the data\n",
    "os.environ['LOKY_MAX_CPU_COUNT'] = '8'\n",
    "X = df.drop(['Price','saledate'], axis=1)\n",
    "y = df['Price']\n",
    "\n",
    "saledate = df['saledate']\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#95 percentile impute\n",
    "X_train, X_test, y_train, y_test, saledate_X,saledate_y = train_test_split(X, y,saledate, test_size=0.2, random_state=42)\n",
    "y_train_upper_cut = upper_percentile_cutoff(95, y,y_train)\n",
    "model = tune_and_train_model(X_train, y_train_upper_cut, X_test, y_test)\n",
    "make_predictions_and_plot(model,y, X_test, y_test, saledate_y,'Actual Prices vs Underestimated Predictions Based on Sale Date on training dataset 95% impute',models,after_training_cutoff=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 percentile subtract\n",
    "X_train, X_test, y_train, y_test, saledate_X,saledate_y = train_test_split(X, y,saledate, test_size=0.2, random_state=42)\n",
    "model = tune_and_train_model(X_train, y_train, X_test, y_test)\n",
    "make_predictions_and_plot(model, y,X_test, y_test, saledate_y,'Actual Prices vs Underestimated Predictions Based on Sale Date on training dataset 5% substract',models,after_training_cutoff=lower_percentile_subtract)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#95 percentile min max scale\n",
    "X_train, X_test, y_train, y_test, saledate_X,saledate_y = train_test_split(X, y,saledate, test_size=0.2, random_state=42)\n",
    "y_train = y_train.values.reshape(-1, 1)\n",
    "percentile_95 = np.percentile(y, 95)\n",
    "y_train_scaled = min_max_scale(y_train, feature_range=(0, percentile_95))\n",
    "model = tune_and_train_model(X_train, y_train_scaled, X_test, y_test)\n",
    "make_predictions_and_plot(model,y, X_test, y_test, saledate_y,'Actual Prices vs Underestimated Predictions Based on Sale Date on training dataset 95% percentile min max scale',models,after_training_cutoff=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the best model\n",
    "key = min(models, key=models.get)\n",
    "joblib.dump(key, 'best_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "- Based on the dataset LightGBM was a good model for predictions as it's capabilites for feature engineering, fast training and robustness of over-fitting\n",
    "\n",
    "- The Root Mean Squared Error (RMSE) is a popular evaluation metric for regression tasks. In this case, lower RMSE indicates better model performance. It is also in the same unit as the results of the regression.\n",
    "\n",
    "- The actual prices and underestimated predictions based on sale date are visualized for each preprocessing method.\n",
    "\n",
    "- The 95 percentile impute method resulted in the lowest RMSE, while the min-max scale method had the highest RMSE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-training",
   "language": "python",
   "name": "model-training"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
